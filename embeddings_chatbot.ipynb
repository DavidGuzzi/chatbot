{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings Chatbot: RAG + Datos Tabulares\n",
    "\n",
    "Este notebook implementa un chatbot que usa embeddings para datos tabulares:\n",
    "1. **Embeddings** de filas de datos como representaciones semÃ¡nticas\n",
    "2. **RAG** (Retrieval Augmented Generation) para encontrar datos relevantes\n",
    "3. **Semantic Search** para responder preguntas precisas\n",
    "\n",
    "**Ventajas:**\n",
    "- Escalable a datasets grandes (10K+ filas)\n",
    "- BÃºsqueda semÃ¡ntica inteligente\n",
    "- Respuestas contextuales especÃ­ficas\n",
    "- Manejo eficiente de memoria/tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ConfiguraciÃ³n del Entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nfrom openai import OpenAI\nimport os\nfrom dotenv import load_dotenv\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom typing import Dict, List, Any, Tuple\nimport json\nimport pickle\nfrom collections import defaultdict\n\n# Cargar variables de entorno\nload_dotenv()\n\n# Verificar API key\napi_key = os.getenv('OPENAI_API_KEY')\nif not api_key:\n    print(\"âŒ Error: OPENAI_API_KEY no encontrada en el archivo .env\")\n    client = None\nelse:\n    try:\n        client = OpenAI(api_key=api_key)\n        print(\"âœ… Cliente OpenAI inicializado correctamente\")\n    except Exception as e:\n        print(f\"âŒ Error al inicializar OpenAI: {e}\")\n        client = None\n\n# Inicializar modelo de embeddings\nprint(\"â³ Cargando modelo de embeddings...\")\nembedding_model = SentenceTransformer('all-MiniLM-L6-v2')\nprint(\"âœ… Modelo de embeddings cargado\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clase Embeddings Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "source": "class EmbeddingsABTestingChatbot:\n    def __init__(self, embedding_model):\n        self.data = None\n        self.embeddings = None\n        self.text_representations = None\n        self.embedding_model = embedding_model\n        self.client = OpenAI(api_key=os.getenv('OPENAI_API_KEY')) if os.getenv('OPENAI_API_KEY') else None\n        self.aggregated_data = None\n        self.aggregated_embeddings = None\n        self.aggregated_texts = None\n    \n    def load_data(self, file_path: str):\n        \"\"\"Carga los datos y crea embeddings\"\"\"\n        if file_path.endswith('.csv'):\n            self.data = pd.read_csv(file_path)\n        elif file_path.endswith(('.xlsx', '.xls')):\n            self.data = pd.read_excel(file_path)\n        \n        print(f\"âœ… Datos cargados: {len(self.data)} filas\")\n        print(f\"   Experimentos: {', '.join(self.data['experimento'].unique())}\")\n        \n        # Crear embeddings de filas individuales y agregadas\n        self._create_embeddings()\n        self._create_aggregated_embeddings()\n    \n    def _create_row_text(self, row) -> str:\n        \"\"\"Convierte una fila en representaciÃ³n textual descriptiva\"\"\"\n        return f\"\"\"\n        Tienda {row['tienda_id']} del experimento {row['experimento']}.\n        Ubicada en regiÃ³n {row['region']}, tipo {row['tipo_tienda']}.\n        Tiene {row['usuarios']} usuarios, {row['conversiones']} conversiones (tasa {row['conversion_rate']}%).\n        Revenue de {row['revenue']:.2f}.\n        \"\"\".strip()\n    \n    def _create_embeddings(self):\n        \"\"\"Crea embeddings para cada fila de datos\"\"\"\n        print(\"â³ Creando embeddings de filas individuales...\")\n        \n        # Convertir filas a texto descriptivo\n        self.text_representations = []\n        for _, row in self.data.iterrows():\n            text = self._create_row_text(row)\n            self.text_representations.append(text)\n        \n        # Crear embeddings\n        self.embeddings = self.embedding_model.encode(self.text_representations, show_progress_bar=True)\n        print(f\"âœ… {len(self.embeddings)} embeddings creados para filas individuales\")\n    \n    def _create_aggregated_embeddings(self):\n        \"\"\"Crea embeddings para datos agregados - CORREGIDO para evitar doble conteo\"\"\"\n        print(\"â³ Creando embeddings agregados...\")\n        \n        self.aggregated_data = []\n        self.aggregated_texts = []\n        \n        # 1. AGREGACIÃ“N PRINCIPAL POR EXPERIMENTO (para totales generales)\n        exp_summary = self.data.groupby('experimento').agg({\n            'usuarios': 'sum',\n            'conversiones': 'sum',\n            'revenue': 'sum'\n        }).round(2)\n        exp_summary['conversion_rate'] = (exp_summary['conversiones'] / exp_summary['usuarios'] * 100).round(2)\n        exp_summary['revenue_per_user'] = (exp_summary['revenue'] / exp_summary['usuarios']).round(2)\n        exp_summary['store_count'] = self.data.groupby('experimento').size()\n        \n        for exp, metrics in exp_summary.iterrows():\n            text = f\"\"\"\n            RESUMEN TOTAL {exp}: {metrics['usuarios']} usuarios totales en {metrics['store_count']} tiendas.\n            Total {metrics['conversiones']} conversiones con tasa {metrics['conversion_rate']}%.\n            Revenue total {metrics['revenue']:.2f}, promedio por usuario {metrics['revenue_per_user']:.2f}.\n            Este es el TOTAL GENERAL para preguntas de totales de usuarios.\n            \"\"\".strip()\n            self.aggregated_texts.append(text)\n            self.aggregated_data.append({\n                'type': 'experiment_total',\n                'experiment': exp,\n                'data': metrics.to_dict(),\n                'is_total': True  # Marca para identificar totales\n            })\n        \n        # 2. ANÃLISIS POR REGIÃ“N (para preguntas especÃ­ficas de regiÃ³n)\n        region_summary = self.data.groupby(['experimento', 'region']).agg({\n            'usuarios': 'sum',\n            'conversiones': 'sum',\n            'revenue': 'sum'\n        }).round(2)\n        region_summary['conversion_rate'] = (region_summary['conversiones'] / region_summary['usuarios'] * 100).round(2)\n        \n        for (exp, region), metrics in region_summary.iterrows():\n            if metrics['usuarios'] > 0:\n                text = f\"\"\"\n                ANÃLISIS REGIONAL: Experimento {exp} en regiÃ³n {region}.\n                {metrics['usuarios']} usuarios, {metrics['conversiones']} conversiones.\n                Tasa de conversiÃ³n {metrics['conversion_rate']}%, revenue {metrics['revenue']:.2f}.\n                Esta es informaciÃ³n POR REGIÃ“N, no para totales generales.\n                \"\"\".strip()\n                self.aggregated_texts.append(text)\n                self.aggregated_data.append({\n                    'type': 'regional_analysis',\n                    'experiment': exp,\n                    'region': region,\n                    'data': metrics.to_dict(),\n                    'is_total': False\n                })\n        \n        # 3. ANÃLISIS POR TIPO DE TIENDA (para preguntas especÃ­ficas de tipo)\n        store_summary = self.data.groupby(['experimento', 'tipo_tienda']).agg({\n            'usuarios': 'sum',\n            'conversiones': 'sum',\n            'revenue': 'sum'\n        }).round(2)\n        store_summary['conversion_rate'] = (store_summary['conversiones'] / store_summary['usuarios'] * 100).round(2)\n        \n        for (exp, store_type), metrics in store_summary.iterrows():\n            if metrics['usuarios'] > 0:\n                text = f\"\"\"\n                ANÃLISIS POR TIPO: Experimento {exp} en tiendas tipo {store_type}.\n                {metrics['usuarios']} usuarios, {metrics['conversiones']} conversiones.\n                Tasa de conversiÃ³n {metrics['conversion_rate']}%, revenue {metrics['revenue']:.2f}.\n                Esta es informaciÃ³n POR TIPO DE TIENDA, no para totales generales.\n                \"\"\".strip()\n                self.aggregated_texts.append(text)\n                self.aggregated_data.append({\n                    'type': 'store_type_analysis',\n                    'experiment': exp,\n                    'store_type': store_type,\n                    'data': metrics.to_dict(),\n                    'is_total': False\n                })\n        \n        # Crear embeddings para datos agregados\n        self.aggregated_embeddings = self.embedding_model.encode(self.aggregated_texts, show_progress_bar=True)\n        print(f\"âœ… {len(self.aggregated_embeddings)} embeddings agregados creados\")\n    \n    def find_relevant_data(self, question: str, top_k: int = 10, use_aggregated: bool = True) -> List[Dict]:\n        \"\"\"Encuentra datos relevantes usando bÃºsqueda semÃ¡ntica - CORREGIDO\"\"\"\n        # Crear embedding de la pregunta\n        question_embedding = self.embedding_model.encode([question])\n        \n        if use_aggregated:\n            similarities = cosine_similarity(question_embedding, self.aggregated_embeddings)[0]\n            top_indices = similarities.argsort()[-top_k:][::-1]\n            \n            relevant_data = []\n            question_lower = question.lower()\n            \n            # FILTRADO INTELIGENTE para preguntas de totales\n            if any(word in question_lower for word in ['total', 'cuÃ¡ntos', 'suma', 'conjunto']):\n                # Para preguntas de totales, SOLO usar datos marcados como 'total'\n                for idx in top_indices:\n                    item_data = self.aggregated_data[idx]\n                    if item_data.get('is_total', False):  # Solo totales generales\n                        relevant_data.append({\n                            'similarity': similarities[idx],\n                            'text': self.aggregated_texts[idx],\n                            'data': item_data\n                        })\n                        if len(relevant_data) >= 3:  # Limitar a pocos resultados para totales\n                            break\n            else:\n                # Para otras preguntas, usar todos los datos relevantes\n                for idx in top_indices:\n                    relevant_data.append({\n                        'similarity': similarities[idx],\n                        'text': self.aggregated_texts[idx],\n                        'data': self.aggregated_data[idx]\n                    })\n        else:\n            # Usar datos de filas individuales\n            similarities = cosine_similarity(question_embedding, self.embeddings)[0]\n            top_indices = similarities.argsort()[-top_k:][::-1]\n            \n            relevant_data = []\n            for idx in top_indices:\n                row_data = self.data.iloc[idx].to_dict()\n                relevant_data.append({\n                    'similarity': similarities[idx],\n                    'text': self.text_representations[idx],\n                    'data': {'type': 'individual_store', 'store_data': row_data}\n                })\n        \n        return relevant_data\n    \n    def determine_search_strategy(self, question: str) -> bool:\n        \"\"\"Determina si usar datos agregados o individuales\"\"\"\n        # Palabras que indican consultas agregadas\n        aggregated_keywords = [\n            'total', 'cuÃ¡ntos', 'suma', 'comparar', 'mejor', 'peor',\n            'promedio', 'general', 'resumen', 'experimento',\n            'todos', 'todas', 'conjunto'\n        ]\n        \n        # Palabras que indican consultas individuales\n        individual_keywords = [\n            'tienda', 'especÃ­fica', 'particular', 'individual',\n            'detalle', 'ejemplo'\n        ]\n        \n        question_lower = question.lower()\n        \n        aggregated_score = sum(1 for keyword in aggregated_keywords if keyword in question_lower)\n        individual_score = sum(1 for keyword in individual_keywords if keyword in question_lower)\n        \n        # Por defecto usar agregados para consultas generales\n        return aggregated_score >= individual_score\n    \n    def generate_context(self, relevant_data: List[Dict]) -> str:\n        \"\"\"Genera contexto optimizado para el LLM - MEJORADO\"\"\"\n        context_parts = [\"DATOS RELEVANTES ENCONTRADOS:\"]\n        \n        # Verificar si hay datos de totales vs anÃ¡lisis detallados\n        total_data = [item for item in relevant_data if item['data'].get('is_total', False)]\n        analysis_data = [item for item in relevant_data if not item['data'].get('is_total', False)]\n        \n        if total_data:\n            context_parts.append(\"\\nğŸ¯ DATOS TOTALES (usar estos para sumas generales):\")\n            for i, item in enumerate(total_data, 1):\n                context_parts.append(f\"[{i}] {item['text']}\")\n                if item['data']['type'] == 'experiment_total':\n                    data_summary = item['data']['data']\n                    context_parts.append(f\"   TOTAL EXACTO: {data_summary.get('usuarios', 'N/A')} usuarios\")\n        \n        if analysis_data and not any(word in relevant_data[0]['text'].lower() for word in ['total', 'resumen total']):\n            context_parts.append(\"\\nğŸ“Š ANÃLISIS DETALLADOS (NO sumar estos):\")\n            for i, item in enumerate(analysis_data[:3], 1):  # Limitar anÃ¡lisis detallados\n                context_parts.append(f\"[{i}] {item['text'][:100]}...\")\n        \n        return \"\\n\".join(context_parts)\n    \n    def chat(self, question: str) -> str:\n        \"\"\"FunciÃ³n principal del chatbot con embeddings\"\"\"\n        if self.data is None:\n            return \"âŒ Primero carga los datos usando load_data()\"\n        \n        if self.client is None:\n            return \"âŒ Cliente OpenAI no disponible\"\n        \n        # 1. Determinar estrategia de bÃºsqueda\n        use_aggregated = self.determine_search_strategy(question)\n        search_type = \"agregados\" if use_aggregated else \"individuales\"\n        print(f\"ğŸ” Usando datos {search_type}\")\n        \n        # 2. Encontrar datos relevantes\n        relevant_data = self.find_relevant_data(question, top_k=8, use_aggregated=use_aggregated)\n        print(f\"ğŸ“Š {len(relevant_data)} elementos relevantes encontrados\")\n        \n        # 3. Generar contexto\n        context = self.generate_context(relevant_data)\n        \n        # 4. Generar respuesta\n        system_prompt = \"\"\"\n        Eres un analista experto en A/B testing. Respondes preguntas basÃ¡ndote en datos relevantes encontrados mediante bÃºsqueda semÃ¡ntica.\n        \n        IMPORTANTE PARA EVITAR DOBLE CONTEO:\n        - Si ves \"DATOS TOTALES\", usa EXACTAMENTE esos nÃºmeros para responder sobre totales\n        - NO sumes datos de diferentes categorÃ­as (regiÃ³n + tipo de tienda)\n        - Los datos regionales y por tipo son para anÃ¡lisis especÃ­ficos, NO para totales generales\n        - Si la pregunta es sobre total de usuarios, busca el nÃºmero marcado como \"TOTAL EXACTO\"\n        - Nunca sumes usuarios por regiÃ³n Y por tipo de tienda (son los mismos usuarios)\n        \n        Para cÃ¡lculos:\n        - Usa los datos marcados como \"TOTAL\" para preguntas generales\n        - Usa anÃ¡lisis detallados solo para comparaciones especÃ­ficas\n        - Si hay inconsistencia, prioriza los datos marcados como totales oficiales\n        \"\"\"\n        \n        user_prompt = f\"\"\"\n        PREGUNTA: {question}\n        \n        CONTEXTO:\n        {context}\n        \n        Responde la pregunta basÃ¡ndote en los datos relevantes. Para totales de usuarios, usa SOLO los datos marcados como totales, NO sumes categorÃ­as.\n        \"\"\"\n        \n        try:\n            response = self.client.chat.completions.create(\n                model=\"gpt-3.5-turbo\",\n                messages=[\n                    {\"role\": \"system\", \"content\": system_prompt},\n                    {\"role\": \"user\", \"content\": user_prompt}\n                ],\n                max_tokens=800,\n                temperature=0.2\n            )\n            return response.choices[0].message.content\n        except Exception as e:\n            return f\"âŒ Error al generar respuesta: {e}\"\n    \n    def save_embeddings(self, file_path: str):\n        \"\"\"Guarda embeddings para uso futuro\"\"\"\n        embedding_data = {\n            'embeddings': self.embeddings,\n            'text_representations': self.text_representations,\n            'aggregated_embeddings': self.aggregated_embeddings,\n            'aggregated_texts': self.aggregated_texts,\n            'aggregated_data': self.aggregated_data\n        }\n        with open(file_path, 'wb') as f:\n            pickle.dump(embedding_data, f)\n        print(f\"âœ… Embeddings guardados en {file_path}\")\n    \n    def load_embeddings(self, file_path: str):\n        \"\"\"Carga embeddings previamente guardados\"\"\"\n        try:\n            with open(file_path, 'rb') as f:\n                embedding_data = pickle.load(f)\n            \n            self.embeddings = embedding_data['embeddings']\n            self.text_representations = embedding_data['text_representations']\n            self.aggregated_embeddings = embedding_data['aggregated_embeddings']\n            self.aggregated_texts = embedding_data['aggregated_texts']\n            self.aggregated_data = embedding_data['aggregated_data']\n            \n            print(f\"âœ… Embeddings cargados desde {file_path}\")\n            return True\n        except Exception as e:\n            print(f\"âŒ Error cargando embeddings: {e}\")\n            return False\n    \n    def show_search_preview(self, question: str, top_k: int = 5):\n        \"\"\"Muestra vista previa de bÃºsqueda semÃ¡ntica\"\"\"\n        use_aggregated = self.determine_search_strategy(question)\n        relevant_data = self.find_relevant_data(question, top_k=top_k, use_aggregated=use_aggregated)\n        \n        print(f\"ğŸ” BÃºsqueda para: '{question}'\")\n        print(f\"ğŸ“Š Estrategia: {'Agregados' if use_aggregated else 'Individuales'}\")\n        print(f\"\\nğŸ¯ Top {len(relevant_data)} resultados mÃ¡s relevantes:\")\n        \n        for i, item in enumerate(relevant_data, 1):\n            is_total = item['data'].get('is_total', False)\n            total_indicator = \" [TOTAL]\" if is_total else \"\"\n            print(f\"\\n[{i}] Relevancia: {item['similarity']:.3f}{total_indicator}\")\n            print(f\"    {item['text'][:100]}...\")\n            if is_total and 'data' in item['data']:\n                usuarios = item['data']['data'].get('usuarios', 'N/A')\n                print(f\"    >>> TOTAL EXACTO: {usuarios} usuarios\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Crear Datos de Prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar los mismos datos base\n",
    "np.random.seed(42)\n",
    "\n",
    "# Datos detallados por tienda\n",
    "tiendas_detalle = []\n",
    "experimentos = ['Control', 'Experimento_A', 'Experimento_B', 'Experimento_C']\n",
    "base_conversion_rates = [8.5, 9.4, 10.2, 11.9]\n",
    "\n",
    "for exp, base_conversion in zip(experimentos, base_conversion_rates):\n",
    "    for i in range(50):  # 50 tiendas por experimento\n",
    "        tiendas_detalle.append({\n",
    "            'experimento': exp,\n",
    "            'tienda_id': f'T_{exp}_{i+1:03d}',\n",
    "            'region': np.random.choice(['Norte', 'Sur', 'Este', 'Oeste']),\n",
    "            'tipo_tienda': np.random.choice(['Mall', 'Street', 'Outlet']),\n",
    "            'usuarios': np.random.randint(150, 300),\n",
    "            'conversiones': None,\n",
    "            'revenue': None,\n",
    "            'conversion_rate': np.random.normal(base_conversion, 2.5)\n",
    "        })\n",
    "\n",
    "tiendas_df = pd.DataFrame(tiendas_detalle)\n",
    "tiendas_df['conversiones'] = (tiendas_df['usuarios'] * tiendas_df['conversion_rate'] / 100).round().astype(int)\n",
    "tiendas_df['revenue'] = tiendas_df['conversiones'] * np.random.uniform(15, 25, len(tiendas_df))\n",
    "tiendas_df['conversion_rate'] = tiendas_df['conversion_rate'].round(2)\n",
    "\n",
    "# Guardar archivo\n",
    "tiendas_df.to_csv('embeddings_tiendas_detalle.csv', index=False)\n",
    "\n",
    "print(\"âœ… Archivo embeddings_tiendas_detalle.csv creado\")\n",
    "print(f\"ğŸ“Š {len(tiendas_df)} filas, {len(experimentos)} experimentos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inicializar Chatbot con Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear chatbot con embeddings\n",
    "embeddings_bot = EmbeddingsABTestingChatbot(embedding_model)\n",
    "\n",
    "# Cargar datos (esto crearÃ¡ automÃ¡ticamente los embeddings)\n",
    "embeddings_bot.load_data('embeddings_tiendas_detalle.csv')\n",
    "\n",
    "# Opcional: Guardar embeddings para uso futuro\n",
    "embeddings_bot.save_embeddings('ab_testing_embeddings.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Ejemplos de Uso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo 1: Total usuarios Control\n",
    "pregunta_1 = \"Â¿CuÃ¡ntos usuarios en total tienen las tiendas control?\"\n",
    "print(f\"ğŸ¤” Pregunta: {pregunta_1}\")\n",
    "print(f\"ğŸ’¬ Respuesta: {embeddings_bot.chat(pregunta_1)}\")\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo 2: Vista previa de bÃºsqueda\n",
    "embeddings_bot.show_search_preview(\"Â¿CuÃ¡ntos usuarios en total tienen las tiendas control?\")\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo 3: Mejor experimento\n",
    "pregunta_3 = \"Â¿CuÃ¡l es el mejor experimento?\"\n",
    "print(f\"ğŸ¤” Pregunta: {pregunta_3}\")\n",
    "print(f\"ğŸ’¬ Respuesta: {embeddings_bot.chat(pregunta_3)}\")\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo 4: AnÃ¡lisis regional\n",
    "pregunta_4 = \"Â¿CÃ³mo se comporta el experimento Control en la regiÃ³n Norte?\"\n",
    "print(f\"ğŸ¤” Pregunta: {pregunta_4}\")\n",
    "embeddings_bot.show_search_preview(pregunta_4)\n",
    "print(f\"\\nğŸ’¬ Respuesta: {embeddings_bot.chat(pregunta_4)}\")\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo 5: ComparaciÃ³n de tipos de tienda\n",
    "pregunta_5 = \"Â¿QuÃ© tipo de tienda funciona mejor en el Experimento_B?\"\n",
    "print(f\"ğŸ¤” Pregunta: {pregunta_5}\")\n",
    "print(f\"ğŸ’¬ Respuesta: {embeddings_bot.chat(pregunta_5)}\")\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ComparaciÃ³n de Estrategias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar bÃºsqueda agregada vs individual\n",
    "test_questions = [\n",
    "    \"Â¿CuÃ¡ntos usuarios totales tiene Control?\",\n",
    "    \"Dame detalles de la tienda T_Control_001\",\n",
    "    \"Â¿CuÃ¡l es el mejor experimento?\",\n",
    "    \"Â¿QuÃ© tienda especÃ­fica tiene mÃ¡s conversiones?\",\n",
    "    \"Compara todos los experimentos\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ”¬ AnÃ¡lisis de estrategias de bÃºsqueda:\")\n",
    "for question in test_questions:\n",
    "    use_aggregated = embeddings_bot.determine_search_strategy(question)\n",
    "    strategy = \"ğŸ“Š Agregados\" if use_aggregated else \"ğŸª Individuales\"\n",
    "    print(f\"   {strategy}: '{question}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Chat Interactivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_interactivo_embeddings():\n",
    "    \"\"\"Chat interactivo con el chatbot de embeddings\"\"\"\n",
    "    print(\"ğŸ¤– Embeddings Chatbot iniciado!\")\n",
    "    print(\"ğŸ’¡ Escribe 'salir' para terminar\")\n",
    "    print(\"ğŸ” Escribe 'buscar: tu pregunta' para ver vista previa de bÃºsqueda\\n\")\n",
    "    \n",
    "    while True:\n",
    "        pregunta = input(\"ğŸ¤” Tu pregunta: \")\n",
    "        \n",
    "        if pregunta.lower() in ['salir', 'exit', 'quit']:\n",
    "            print(\"ğŸ‘‹ Â¡Hasta luego!\")\n",
    "            break\n",
    "        \n",
    "        if pregunta.lower().startswith('buscar:'):\n",
    "            search_query = pregunta[7:].strip()\n",
    "            embeddings_bot.show_search_preview(search_query)\n",
    "            continue\n",
    "            \n",
    "        if pregunta.strip() == \"\":\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nğŸ’¬ Respuesta: {embeddings_bot.chat(pregunta)}\")\n",
    "        print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "# Descomentar para usar el chat interactivo\n",
    "# chat_interactivo_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. AnÃ¡lisis de Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AnÃ¡lisis de similitudes para diferentes preguntas\n",
    "test_questions_analysis = [\n",
    "    \"Â¿CuÃ¡ntos usuarios control?\",\n",
    "    \"Total de usuarios en Control\",\n",
    "    \"Usuarios grupo control\",\n",
    "    \"Â¿CuÃ¡l es el mejor experimento?\",\n",
    "    \"Experimento mÃ¡s exitoso\",\n",
    "    \"Â¿QuÃ© funciona mejor?\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ“ˆ AnÃ¡lisis de similitud semÃ¡ntica:\")\n",
    "for question in test_questions_analysis:\n",
    "    relevant_data = embeddings_bot.find_relevant_data(question, top_k=3)\n",
    "    print(f\"\\nğŸ” '{question}'\")\n",
    "    for i, item in enumerate(relevant_data[:2], 1):\n",
    "        print(f\"   [{i}] {item['similarity']:.3f} - {item['text'][:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. VerificaciÃ³n de PrecisiÃ³n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar que los embeddings devuelven datos correctos\n",
    "print(\"ğŸ” VerificaciÃ³n de precisiÃ³n - Total usuarios Control:\")\n",
    "\n",
    "# CÃ¡lculo manual\n",
    "manual_control = tiendas_df[tiendas_df['experimento'] == 'Control']['usuarios'].sum()\n",
    "print(f\"ğŸ“Š Manual: {manual_control} usuarios\")\n",
    "\n",
    "# Buscar con embeddings\n",
    "relevant_data = embeddings_bot.find_relevant_data(\"total usuarios control\", top_k=5)\n",
    "print(f\"\\nğŸ¯ Datos mÃ¡s relevantes encontrados:\")\n",
    "for item in relevant_data[:3]:\n",
    "    if item['data']['type'] == 'experiment_summary' and item['data']['experiment'] == 'Control':\n",
    "        usuarios_embeddings = item['data']['data']['usuarios']\n",
    "        print(f\"ğŸ¤– Embeddings: {usuarios_embeddings} usuarios\")\n",
    "        print(f\"âœ… Coinciden: {manual_control == usuarios_embeddings}\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nğŸ“‹ Top 3 resultados por relevancia:\")\n",
    "for i, item in enumerate(relevant_data[:3], 1):\n",
    "    print(f\"[{i}] Sim: {item['similarity']:.3f} - {item['text'][:80]}...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}